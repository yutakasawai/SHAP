{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#最終課題"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##機能要件\n",
    "<br>\n",
    "・2値の分類タスクを扱える<br>\n",
    "-カテゴリカル変数を指定すると oneone -hot エンコードを実行する<br>\n",
    "モデル用データマートに施したのと同一前処理をスコア用データマートに対しても適用される<br>\n",
    "モデル選択の評価指標を選択できる<br>\n",
    "複数アルゴリズムから指定の評価指標に従いベストモデルを選択できる<br>\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html\n",
    "http://scikit-learn.org/stable/modules/model_evaluation.html#scoring-parameter\n",
    "学習済みモデルを保存できる<br>\n",
    " アルゴリズムランキングと性能評価指標が出力される<br>\n",
    " 学習済みモデル（保存したモデル）を呼び出しスコア用データに対して予測確率を付与できる<br>\n",
    "\n",
    "\n",
    "\n",
    "開発課題認定プロセス（最終日にその場で実行・提出）<br>\n",
    " 訓練用データと（正解の無い）検証を配布します<br>\n",
    " データは第一カラムが ID 、第二カラムがクラス変数、三以降特徴ベクトルの構成です。<br>\n",
    " クラス変数は 0/1 のバイナリ値で予測確率を知りたいクラスは ”1” です。<br>\n",
    " ID （第一カラム）と予測確率（第二カラム）の２カラムの構成結果ファイルを ヘッダーありのCSV形式で提出2値の分類タスクを扱える<br>\n",
    " カテゴリル変数を指定すると oneone -hot エンコードを実行する<br>\n",
    " モデル用データマートに施したのと同一前処理をスコア用データマートに対しても適用される<br>\n",
    " モデル選択の評価指標を選択できる<br>\n",
    " 複数アルゴリズムから指定の評価標に従いベストモデルを選択できる<br>\n",
    " 学習済みモデルを保存できるP05S01<br>\n",
    " アルゴリズムランキングと性能評価指標が出力される<br>\n",
    " 学習済みモデル（保存したモデル）を呼び出しスコア用データに対して予測確率を付与できる<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start=time.time()\n",
    "print(start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4500 entries, 0 to 4499\n",
      "Data columns (total 22 columns):\n",
      "satisfaction_level       4500 non-null float64\n",
      "last_evaluation          4500 non-null float64\n",
      "number_project           4500 non-null float64\n",
      "average_montly_hours     4500 non-null float64\n",
      "time_spend_company       4500 non-null float64\n",
      "Work_accident            4500 non-null float64\n",
      "promotion_last_5years    4500 non-null float64\n",
      "sales_IT                 4500 non-null float64\n",
      "sales_RandD              4500 non-null float64\n",
      "sales_accounting         4500 non-null float64\n",
      "sales_hr                 4500 non-null float64\n",
      "sales_management         4500 non-null float64\n",
      "sales_marketing          4500 non-null float64\n",
      "sales_product_mng        4500 non-null float64\n",
      "sales_sales              4500 non-null float64\n",
      "sales_support            4500 non-null float64\n",
      "sales_technical          4500 non-null float64\n",
      "sales_nan                4500 non-null float64\n",
      "salary_high              4500 non-null float64\n",
      "salary_low               4500 non-null float64\n",
      "salary_medium            4500 non-null float64\n",
      "salary_nan               4500 non-null float64\n",
      "dtypes: float64(22)\n",
      "memory usage: 773.5 KB\n",
      "モデルのみに存在する項目: set()\n",
      "スコアのみに存在する項目: set()\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "import warnings\n",
    "#warnings.filterwarnings('ignore')\n",
    "\n",
    "#import scipy as scp\n",
    "from scipy.stats import randint as sp_randint\n",
    "\n",
    "#標準化とパイプライン　その他の便利ツール\n",
    "from sklearn.preprocessing import StandardScaler,Imputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_selection import RFE,RFECV\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy import stats #アルゴリズムランキング\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE\n",
    "\n",
    "#アルゴリズム\n",
    "from sklearn.neighbors import KNeighborsClassifier #KNN\n",
    "from sklearn.linear_model import LogisticRegression #ロジスティクス回帰\n",
    "from sklearn.ensemble import GradientBoostingClassifier,RandomForestClassifier,VotingClassifier,BaggingClassifier #勾配ブースティング ランダムフォレスト\n",
    "from sklearn.neural_network import MLPClassifier #多層ニューラルネットワーク\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from xgboost.sklearn import XGBClassifier\n",
    "import catboost as cb\n",
    "from catboost import CatBoostClassifier\n",
    "import lightgbm as lgb\n",
    "\n",
    "#評価方法\n",
    "from sklearn.model_selection import train_test_split,cross_val_score,KFold,ShuffleSplit#交差検証,K-fold、シャッフル分割交差検証\n",
    "#評価指標\n",
    "from sklearn.metrics import accuracy_score,f1_score,roc_auc_score\n",
    "#ファイル保存\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "from contextlib import contextmanager\n",
    "import time\n",
    "@contextmanager\n",
    "def timer(name):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(f'[{name}] done in {time.time() - t0:.0f} s')\n",
    "    \n",
    "#k-fold shuffle\n",
    "\n",
    "kfold=KFold(n_splits=20,shuffle=True,random_state=0)\n",
    "shuffle_split=ShuffleSplit(test_size=.7,train_size=.3,n_splits=1,random_state=0)\n",
    "\"\"\"\n",
    "評価手法\n",
    "'accuracy'\n",
    "'average_precision'\n",
    "'f1'\n",
    "'f1_micro'\n",
    "'f1_macro'\n",
    "'f1_weighted'\n",
    "'f1_samples'\n",
    "'neg_log_loss'\n",
    "'precision'\n",
    "'recall'\n",
    "'roc_auc'\n",
    "\"\"\"\n",
    "#scoring_method='accuracy'\n",
    "scoring_method='roc_auc'\n",
    "\n",
    "df = pd.read_csv('./data/final_hr_analysis_train.csv', header=0)\n",
    "df_s = pd.read_csv('./data/final_hr_analysis_test.csv', header=0)\n",
    "ohe_columns =['sales','salary']\n",
    "\n",
    "def get_ohe(X,ohe_columns):\n",
    "    ohe_columns =['sales','salary']\n",
    "\n",
    "    #get_dummiesでダミー変数化\n",
    "    X_ohe = pd.get_dummies(X, dummy_na=True, columns=ohe_columns)\n",
    "    X_ohe_columns=X_ohe.columns.values\n",
    "    return  X_ohe, X_ohe_columns\n",
    "\n",
    "\n",
    "def preprocessing_data(df,df_S):\n",
    "      #訓練用データ読み込み\n",
    "    X = df.iloc[:, 2:]            #\n",
    "    X = pd.DataFrame(X, columns=X.columns)\n",
    "    #X = X.drop('', axis=1)  # \n",
    "    y = df.iloc[:, [1]]           # 2列目を正解データとして読込\n",
    "    y=pd.DataFrame(y,columns=y.columns)\n",
    "    #Kaggle titanic用\n",
    "    #drop_in=['Name','Cabin','Ticket']#名前。キャビンはNULLが多いので削除する\n",
    "    #X=X.drop(drop_in,axis=1)\n",
    "    \n",
    "    X_ohe ,X_ohe_columns = get_ohe(X,ohe_columns)\n",
    "    \n",
    "    #訓練用データの特徴量を選択して整形する前処理。\n",
    "    imp=Imputer(missing_values='NaN',strategy='mean',axis=0)\n",
    "    imp.fit(X_ohe)\n",
    "    X_ohe=pd.DataFrame(imp.transform(X_ohe),\n",
    "                             columns=X_ohe_columns)\n",
    "    #RFEで特徴量を選択\n",
    "    selector = RFECV(estimator=XGBClassifier(),cv=shuffle_split,step=1,scoring=scoring_method)#XGBCはdefaultでrandom_state=0\n",
    "    #selector = RFECV(estimator=LogisticRegression(random_state=1),cv=shuffle_split,step=0.05)#XGBCはdefaultでrandom_state=0\n",
    "    selector.fit(X_ohe, y.as_matrix().ravel())\n",
    "    #訓練データの最終形\n",
    "    X_fin=pd.DataFrame(X_ohe,columns=X_ohe.columns.values[selector.ranking_==1])    \n",
    "    X_fin=X_fin.astype(float)\n",
    "    #テストデータの読み込み\n",
    "    #df_s = pd.read_csv('./data/test.csv', header=0)\n",
    "    X_s = df_s.iloc[:, 1:]\n",
    "    X_s=pd.DataFrame(X_s, columns=X_s.columns)\n",
    "    #X_s=X_s.drop(drop_in,axis=1)\n",
    "    #ID_s =df_s.iloc[:, [0]]\n",
    "    ID_s =df_s['index']\n",
    "    X_ohe_s,X_ohe_columns_s  =get_ohe(X_s,ohe_columns)\n",
    "    X_ohe_s=X_ohe_s.astype(float)\n",
    "    X_ohe_s.info()\n",
    "    cols_model = set(X_ohe.columns.values)\n",
    "    cols_score = set(X_ohe_s.columns.values)\n",
    "\n",
    "    # モデルにはあったがスコアにはないデータ項目\n",
    "    diff1 = cols_model - cols_score\n",
    "    print('モデルのみに存在する項目: %s' % diff1)\n",
    "\n",
    "    # スコアにはあるがモデルになかったデータ項目\n",
    "    diff2 = cols_score - cols_model\n",
    "    print('スコアのみに存在する項目: %s' % diff2)\n",
    "    #訓練データのカラムを作成\n",
    "    df_cols_m = pd.DataFrame(None, columns=X_ohe_columns, dtype=float)\n",
    "    df_cols_m\n",
    "    \n",
    "    #訓練データにしかないカラムを補完する\n",
    "    X_ohe_s2=pd.concat([df_cols_m,X_ohe_s])\n",
    "    X_ohe_s2.head()\n",
    "    \n",
    "    #スコアデータのみのカラムを削除する\n",
    "    X_ohe_s2 = X_ohe_s2.drop(list(set(X_ohe_s.columns.values)\n",
    "                                  -set(X_ohe.columns.values))\n",
    "                             , axis=1)\n",
    "    #訓練データのみのデータ項目を0で埋める\n",
    "    X_ohe_s2.loc[:,list(set(X_ohe.columns.values)-set(X_ohe_s.columns.values))] = \\\n",
    "    X_ohe_s2.loc[:,list(set(X_ohe.columns.values)-set(X_ohe_s.columns.values))].fillna(0, axis=1)\n",
    "    \n",
    "    #並び順の整形\n",
    "    X_ohe_s2 = X_ohe_s2.reindex_axis(X_ohe.columns.values, axis=1)\n",
    "    #モデリング時のimputerを使用して欠損地補完\n",
    "    X_ohe_s3 = pd.DataFrame(imp.transform(X_ohe_s2), columns=X_ohe_columns)\n",
    "    #テストデータと同じカラムに絞り込みをする\n",
    "    X_fin_s = X_ohe_s3.loc[:, X_ohe.columns.values[selector.support_]]\n",
    "    return X_fin,y,X_fin_s,ID_s\n",
    "    \n",
    "X_fin,y,X_fin_s,ID_s = preprocessing_data(df,df_s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_fin_s shape: (4500,22)\n",
      "X_fin shape: (10499,22)\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4500 entries, 0 to 4499\n",
      "Data columns (total 22 columns):\n",
      "satisfaction_level       4500 non-null float64\n",
      "last_evaluation          4500 non-null float64\n",
      "number_project           4500 non-null float64\n",
      "average_montly_hours     4500 non-null float64\n",
      "time_spend_company       4500 non-null float64\n",
      "Work_accident            4500 non-null float64\n",
      "promotion_last_5years    4500 non-null float64\n",
      "sales_IT                 4500 non-null float64\n",
      "sales_RandD              4500 non-null float64\n",
      "sales_accounting         4500 non-null float64\n",
      "sales_hr                 4500 non-null float64\n",
      "sales_management         4500 non-null float64\n",
      "sales_marketing          4500 non-null float64\n",
      "sales_product_mng        4500 non-null float64\n",
      "sales_sales              4500 non-null float64\n",
      "sales_support            4500 non-null float64\n",
      "sales_technical          4500 non-null float64\n",
      "sales_nan                4500 non-null float64\n",
      "salary_high              4500 non-null float64\n",
      "salary_low               4500 non-null float64\n",
      "salary_medium            4500 non-null float64\n",
      "salary_nan               4500 non-null float64\n",
      "dtypes: float64(22)\n",
      "memory usage: 773.5 KB\n"
     ]
    }
   ],
   "source": [
    "print('X_fin_s shape: (%i,%i)' %X_fin_s.shape)\n",
    "print('X_fin shape: (%i,%i)' %X_fin.shape)\n",
    "X_fin_s.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "left\n",
      "0    7966\n",
      "1    2533\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(y.groupby('left').size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# resampling\n",
    "rus = RandomUnderSampler(random_state=0)\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "smt = SMOTE(random_state=0)\n",
    "\n",
    "pipe_knn = Pipeline([('scl',StandardScaler()),('est',KNeighborsClassifier())])\n",
    "pipe_logistic = Pipeline([('scl',StandardScaler()),('est',LogisticRegression(random_state=1))])\n",
    "pipe_rf = Pipeline([('scl',StandardScaler()),('est',RandomForestClassifier(random_state=1))])\n",
    "pipe_gbc = Pipeline([('scl',StandardScaler()),('est',GradientBoostingClassifier(random_state=1))])\n",
    "#pipe_mlp = Pipeline([('scl',StandardScaler()),('est',MLPClassifier(hidden_layer_sizes=(5,2), max_iter=2000, random_state=1))])\n",
    "pipe_x_clf = Pipeline([('scl',StandardScaler()),('est',XGBClassifier())])\n",
    "#pipe_cb=Pipeline([('scl',StandardScaler()),('est',cb.CatBoostClassifier())])\n",
    "pipe_lgb=Pipeline([('scl',StandardScaler()),('est',lgb.LGBMClassifier(random_state=1))])\n",
    "\n",
    "pipe_vt=Pipeline([('scl',StandardScaler()),('est',VotingClassifier(estimators=[('rf',pipe_rf), ('gbc',pipe_gbc),('x_clf',pipe_x_clf)], voting='soft',weights=[3,1,1]))])\n",
    "pipe_bg=Pipeline([('scl',StandardScaler()),('est',BaggingClassifier(base_estimator=RandomForestClassifier(random_state=1),random_state=1))])\n",
    "#pipeの名前リスト\n",
    "#pipe_names=['KNN','Logistic','RandomForest','GradientBoosting','XGBClassifier','LGBMClassifier','VotingClassifier','BaggingClassifier']\n",
    "\n",
    "pipe_names=['RandomForest','GradientBoosting','XGBClassifier','LGBMClassifier','VotingClassifier','BaggingClassifier']\n",
    "#pipelineのリスト\n",
    "pipe_lines=[pipe_rf,pipe_gbc,pipe_x_clf,pipe_lgb,pipe_vt,pipe_bg]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------\n",
      "探索空間:{'est__n_estimators': [100, 200, 300], 'est__max_depth': [3, None], 'est__bootstrap': [True]}\n",
      "Best Score: 0.985356\n",
      "\n",
      "Best Model: Pipeline(memory=None,\n",
      "     steps=[('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), ('est', RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n",
      "            max_depth=None, max_features='auto', max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, n_estimators=100, n_jobs=1,\n",
      "            oob_score=False, random_state=1, verbose=0, warm_start=False))])\n",
      "[RandomForest] done in 4 s\n",
      "----------------------------------------------------------------------------------------------\n",
      "探索空間:{'est__n_estimators': [100, 200, 300], 'est__subsample': [0.8, 1.0]}\n",
      "Best Score: 0.988568\n",
      "\n",
      "Best Model: Pipeline(memory=None,\n",
      "     steps=[('scl', StandardScaler(copy=True, with_mean=True, with_std=True)), ('est', GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
      "              learning_rate=0.1, loss='deviance', max_depth=3,\n",
      "              max_features=None, max_leaf_nodes=None,\n",
      "              min_impurity_decrease=0...         presort='auto', random_state=1, subsample=1.0, verbose=0,\n",
      "              warm_start=False))])\n",
      "[GradientBoosting] done in 5 s\n",
      "----------------------------------------------------------------------------------------------\n",
      "探索空間:{'est__learning_rate': [0.1], 'est__n_estimators': [100, 200, 300], 'est__max_depth': [1, 3, 5, 7], 'est__min_child_weight': [1, 2, 3], 'est__max_delta_step': [5], 'est__gamma': [0, 3, 10, 15], 'est__subsample': [0.8], 'est__colsample_bytree': [0.8], 'est__objective': ['binary:logistic']}\n"
     ]
    }
   ],
   "source": [
    "param_grid_knn = {'est__n_neighbors': range(1, 20),\n",
    "                'est__weights': [\"uniform\", \"distance\"],\n",
    "                'est__p': [1, 2]}\n",
    "\n",
    "param_grid_logistic = {'est__penalty':['l1','l2'],\n",
    "                       'est__class_weight':['balanced',{1:1,0:1},{1:2,0:1},{1:4,0:1},{1:10,0:1}],\n",
    "                        'est__C': [1e-4, 1e-3, 1e-2, 1e-1, 0.5, 1., 5., 10., 15., 20., 25.]\n",
    "                        }\n",
    "\n",
    "param_grid_rf = {\"est__n_estimators\":[100,200,300],\n",
    "              \"est__max_depth\": [3, None],  \n",
    "              \"est__bootstrap\": [True]}\n",
    "\n",
    "param_grid_gbc = {'est__n_estimators':[100,200,300],\n",
    "                  'est__subsample':[0.8, 1.0]}\n",
    "\n",
    "\"\"\"\n",
    "param_grid_mlp = {'est__activation':['identity', 'logistic', 'tanh', 'relu']\n",
    "                  ,'est__alpha' :[0.0001,0.001, 0.01,0.1]\n",
    "                ,'est__solver':['lbfgs','sgd','adam']\n",
    "              #  ,'est__learning_rate' : {'constant', 'invscaling', 'adaptive'}\n",
    "}\n",
    "\"\"\"    \n",
    "\n",
    "param_grid_x_clf={'est__learning_rate':[0.1],\n",
    "'est__n_estimators':[100,200,300],\n",
    "'est__max_depth':[1,3,5,7],\n",
    "'est__min_child_weight':[1,2,3],\n",
    "'est__max_delta_step':[5],\n",
    "'est__gamma':[0,3,10,15],\n",
    "'est__subsample':[0.8],\n",
    "'est__colsample_bytree':[0.8],\n",
    "'est__objective':['binary:logistic']\n",
    "                  \n",
    "}\n",
    "\n",
    "\"\"\"\n",
    "param_grid_cb ={'est__depth': [4, 7, 10],\n",
    "         # 'est__learning_rate' : [0.03, 0.1, 0.15],\n",
    "         #'est__l2_leaf_reg': [1,4,9],\n",
    "         'est__iterations': [300]}\n",
    "\"\"\"\n",
    "\n",
    "param_grid_gbm = {\n",
    "  #  'est__learning_rate': [0.005],\n",
    "    'est__n_estimators': [100,200,300,400],\n",
    "   # 'est__num_leaves': [6,8,12,16],\n",
    "    'est__boosting_type' : ['gbdt','dart','goss'],\n",
    "    'est__objective' : ['binary'],\n",
    "    'est__random_state' : [501], \n",
    "    'est__tree_learner':['feature']\n",
    "    }\n",
    "param_vt={}\n",
    "\"\"\"\n",
    "param_vt.update({\"est__rf__\" + k: v for k, v in param_grid_rf.items()})\n",
    "param_vt.update({\"est__gbc__\" + k: v for k, v in param_grid_gbc.items()})\n",
    "param_vt.update({\"est__x_clf__\" + k: v for k, v in param_grid_x_clf.items()})\n",
    "\"\"\"\n",
    "\n",
    "param_bg={'est__n_estimators': [10,20,100,200,300],\n",
    "              #\"est__base_estimator__n_estimators\":[100,200,300],\n",
    "              #\"est__base_estimator__max_depth\": [3, None],\n",
    "              #\"est__base_estimator__bootstrap\": [True]\n",
    "         }\n",
    "          \n",
    "\n",
    "\n",
    "params = [param_grid_rf , param_grid_gbc,param_grid_x_clf,param_grid_gbm,param_vt,param_bg]\n",
    "best_estimator = []\n",
    "best_score=0\n",
    "i=0\n",
    "j=0\n",
    "alg_ranking = pd.DataFrame(index=[], columns=['name','score'])\n",
    "\n",
    "for i,elem in enumerate(zip(pipe_lines, params,pipe_names)):\n",
    "    print('----------------------------------------------------------------------------------------------')\n",
    "    pipe, param,pipe_name = elem[0], elem[1],elem[2]\n",
    "    with timer(pipe_name):\n",
    "        \n",
    "        print('探索空間:%s' % param)\n",
    "    #print('探索空間:%s' % pipe_names[i])\n",
    "    #gs = GridSearchCV(estimator=pipe, param_grid=param, scoring='f1', cv=shuffle_split)\n",
    "        gs = GridSearchCV(estimator=pipe, param_grid=param, scoring=scoring_method, cv=shuffle_split)\n",
    "    \n",
    "        gs = gs.fit(X_fin, y.as_matrix().ravel())\n",
    "        best_estimator.append(gs.best_estimator_)   # gs.best_estimator_でベストモデルを呼び出せる\n",
    "        print('Best Score: %.6f\\n' % gs.best_score_) # gs.best_score_で上記ベストモデルのCV評価値を呼び出せる\n",
    "        print('Best Model: %s' % gs.best_estimator_) #cv_results_にも結果が格納されている。\n",
    "        series = pd.Series([pipe_name,gs.best_score_], index=alg_ranking.columns)\n",
    "        alg_ranking = alg_ranking.append(series, ignore_index = True)\n",
    "        \n",
    "    \n",
    "    if gs.best_score_ > best_score:\n",
    "        best_score = gs.best_score_\n",
    "        best_index = i\n",
    "\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "print('ベストスコアは' + pipe_names[best_index]+'で'+str(best_score))\n",
    "print('----------------------------------------------------------------------------------------')\n",
    "\n",
    "\n",
    "        \n",
    "for j, est in enumerate(best_estimator):\n",
    "    joblib.dump(est, pipe_names[j] + '.pkl')     \n",
    "alg_ranking = alg_ranking.assign(ranking=len(alg_ranking.score)-stats.mstats.rankdata(alg_ranking.score)+1)\n",
    "alg_ranking = alg_ranking[['ranking', 'name','score']].sort_values('ranking',ascending=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alg_ranking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bestmodel_name=alg_ranking['name'].loc[alg_ranking['ranking']==1].values[0]\n",
    "loaded_model = joblib.load(bestmodel_name+'.pkl')\n",
    "print(bestmodel_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prediction_proba = loaded_model.predict_proba(X_fin_s)[:,[1]].flatten()\n",
    "prediction = loaded_model.predict(X_fin_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_proba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Submission = pd.DataFrame({ 'id': ID_s,'prediction':prediction_proba})\n",
    "#StackingSubmission_=StackingSubmission.astype({'id':int, 'prediction': float})\n",
    "Submission.to_csv(\"aijc1115.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "process_time = time.time() - start\n",
    "print(process_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
